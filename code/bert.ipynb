{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TBgnQlDStqKa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psnEcT_Htr10",
    "outputId": "fa4d0ba3-31fe-481c-d9df-066d67e05356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i reminded him that i am a woman  he complained that he didnt feel listened to\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# REMOVE CERTAIN ABBREVIATIONS\n",
    "def remove_abbreviation(text):\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"'ll\", \" will\")  \n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    \n",
    "    text = text.replace(\"'ve\", \" have\")  \n",
    "    text = text.replace(\"have't\", \"have not\")\n",
    "    \n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    \n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"did't\", \"did not\")\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    return text\n",
    "\n",
    "def filtered(text):\n",
    "    # REFERENCE: https://stackoverflow.com/questions/28840908/perfect-regex-for-extracting-url-with-re-findall\n",
    "    # REMOVE URLS \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"\", text)\n",
    "    # REMOVE @...(@USERS)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    # REMOVE ENDINGS\n",
    "    text = text.replace(\"<URL>\", '')\n",
    "    text = text.lower()\n",
    "    text = remove_abbreviation(text)\n",
    "    # REMOVE PUNCTUATIONS\n",
    "    text = re.sub(\"[,.\\\"\\'!@#$%^&*(){}+=-_?/;`~:<>\\\\\\[\\]]\", \"\", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def get_data():\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "    with open('data/train.jsonl') as json_file: \n",
    "        for i in json_file:\n",
    "            data = json.loads(i)\n",
    "            if (data['label']) == 'SARCASM':\n",
    "              train_labels.append(1)\n",
    "            else:\n",
    "              train_labels.append(0)\n",
    "            train_text.append(filtered(data[\"response\"]))\n",
    "\n",
    "    with open('data/test.jsonl') as json_file: \n",
    "        for i in json_file:\n",
    "            data = json.loads(i)\n",
    "            test_labels.append(int(data['id'].split(\"_\")[1]))\n",
    "            test_text.append(filtered(data[\"response\"]))\n",
    "    return train_text, train_labels, test_text, test_labels\n",
    "\n",
    "\n",
    "train_text, train_labels, test_text, test_labels = get_data()\n",
    "# GET EVAL TEXT AND LABELS\n",
    "eval_text = train_text[:500] + train_text[-500:]\n",
    "eval_labels = train_labels[:500] + train_labels[-500:]\n",
    "# GET TRAIN TEXT AND LABELS\n",
    "train_text = train_text[500:-500]\n",
    "train_labels = train_labels[500:-500]\n",
    "\n",
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "_DZjYqHgtzTN",
    "outputId": "a00ada9b-505f-4c93-fe2a-3fce210e9e27"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "# LOAD PRE-TRAINED BERT\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IUxJWfhbvFl1"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWTaIi6avIu_",
    "outputId": "3c1f2d2b-9607-4572-9d5a-dc355b2c041f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AzureUser/anaconda3/envs/jt_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# REFERENCE: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "\n",
    "# INTIALIZE PRE-TRAINED TOKENIZER\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# TOKENIZE TRAIN\n",
    "tokens_train = tokenizer(\n",
    "    train_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# TOKENIZE VALIDATION\n",
    "tokens_eval = tokenizer(\n",
    "    eval_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# TOKENIZE TEST\n",
    "tokens_test = tokenizer(\n",
    "    test_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe3Ftid3wqlC",
    "outputId": "a26dcfde-f2a1-47fc-a433-9a89aa197d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 80])\n",
      "torch.Size([4000])\n",
      "torch.Size([1000, 80])\n",
      "torch.Size([1000])\n",
      "tensor([  101,  4922,  8040, 21886,  2480,  7164, 24829,  5602,  2003,  3424,\n",
      "         1011, 19640, 24106,  2903, 24829,  5602,  5836, 16498,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# REFERENCE: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "\n",
    "# GET IDS AND MASK FOR TRAIN, VALIDATION and TEST\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "print(train_seq.shape)\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels).to(device)\n",
    "print(train_y.shape)\n",
    "\n",
    "eval_seq = torch.tensor(tokens_eval['input_ids'])\n",
    "print(eval_seq.shape)\n",
    "eval_mask = torch.tensor(tokens_eval['attention_mask'])\n",
    "eval_y = torch.tensor(eval_labels).to(device)\n",
    "print(eval_y.shape)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "print(train_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "\n",
    "# CONSTRUCT DATA LOADER\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "eval_data = TensorDataset(eval_seq, eval_mask, eval_y)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREEZE BERT PARAMETERS\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j1-kYrHxwLzQ"
   },
   "outputs": [],
   "source": [
    "# ACTUAL MODEL FOR FINE-TUNE\n",
    "class BERT_FT(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_FT, self).__init__()\n",
    "      self.bert = bert \n",
    "    \n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "      self.relu =  nn.ReLU()\n",
    "    \n",
    "      self.fc1 = nn.Linear(768,256)\n",
    "      self.fc2 = nn.Linear(256,2)\n",
    "\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "      output= self.bert(sent_id, attention_mask=mask)\n",
    "      x = self.fc1(self.dropout(output.last_hidden_state[:,0,:]))\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MoTxmAGuwOfU"
   },
   "outputs": [],
   "source": [
    "model = BERT_FT(bert)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "g1FbnA0NS4hU"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# DEFINE OPTIMIZER\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vKRvdpGhwRdI"
   },
   "outputs": [],
   "source": [
    "# REFERENCE: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "\n",
    "  total_preds=[]\n",
    "  total_labels = []\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    batch = [r.to(device) for r in batch]\n",
    "    \n",
    "    # GET BATCH DATA\n",
    "    sent_id, mask, labels = batch\n",
    "    optimizer.zero_grad()        \n",
    "\n",
    "    preds = model(sent_id, mask)\n",
    "    # GET PREDICTION LOGITS\n",
    "    loss = criterion(preds, labels)\n",
    "    total_loss = total_loss + loss.item()\n",
    "    # LOSS BACK PROPAGATION\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    total_preds.append(preds.argmax(1))\n",
    "    total_labels.append(labels.cpu().numpy())\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "  return avg_loss, total_preds, np.concatenate(total_labels, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_eval(model):\n",
    "  \n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  total_preds=[]\n",
    "  total_labels = []\n",
    "  for step,batch in enumerate(eval_dataloader):\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "    \n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    total_preds.append(preds.argmax(1))\n",
    "    total_labels.append(labels.cpu().numpy())\n",
    "\n",
    "  \n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return total_preds, np.concatenate(total_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "PZeLMF7ywcvH",
    "outputId": "0ded17bd-385b-48da-b12a-b2c2aa3799c9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n",
      "  Batch   100  of    125.\n",
      "train_acc 0.68875\n",
      "eval_f1 0.694048616932104\n",
      "eval_acc 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AzureUser/anaconda3/envs/jt_env/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BERT_FT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.579\n",
      "\n",
      " Epoch 2 / 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-23b4bd29d66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n Epoch {:} / {:}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_preds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtotal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-65c4fbc622b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "epochs= 20\n",
    "best_valid_loss = float('inf')\n",
    "best_f1 = -10\n",
    "train_losses=[]\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss, total_preds, total_labels = train()\n",
    "    print(\"train_acc\", sum(total_preds == total_labels)/total_labels.shape[0])\n",
    "    preds, eval_labels = evaluate_eval(model)\n",
    "    curr_f1 = f1_score(eval_labels, preds)\n",
    "    print(\"eval_f1\", curr_f1)\n",
    "    print(\"eval_acc\", sum(eval_labels == preds)/preds.shape[0])\n",
    "    if curr_f1 > best_f1:\n",
    "      torch.save(model, \"model123456.pt\")\n",
    "      best_f1 = curr_f1\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate(model):\n",
    "  # GET TEST PREDICTION\n",
    "  model.eval()\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds=preds.detach().argmax(1).cpu().numpy()\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29PCPd1cwgCP"
   },
   "outputs": [],
   "source": [
    "# LOAD BEST MODEL\n",
    "model = torch.load(\"model123456.pt\")\n",
    "# GET PREDICTIONS FOR VALIDATION SET\n",
    "preds, eval_labels = evaluate_eval(model)\n",
    "curr_f1 = f1_score(eval_labels, preds)\n",
    "print(\"eval_f1\", curr_f1)\n",
    "print(\"eval_acc\", sum(eval_labels == preds)/preds.shape[0])\n",
    "with torch.no_grad():\n",
    "  # GET PREDICTIONS FOR TEST SET\n",
    "  preds = evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoIhaHuT2DA1"
   },
   "outputs": [],
   "source": [
    "assert len(preds) == len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOXQ5bqb2Ke6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT TO ANSWER.TXT\n",
    "f = open(\"answer.txt\", 'w+')\n",
    "for i in range(len(test_labels)):\n",
    "    if preds[i] == 0:\n",
    "        curr_pred = \"NOT_SARCASM\"\n",
    "    else:\n",
    "        curr_pred = \"SARCASM\"\n",
    "    f.write('twitter_{},{}\\n'.format(test_labels[i], curr_pred))\n",
    "    print('twitter_{},{}\\n'.format(test_labels[i], curr_pred))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7caYsHY2P2Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
