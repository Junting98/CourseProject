{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBgnQlDStqKa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "torch.manual_seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psnEcT_Htr10",
    "outputId": "fa4d0ba3-31fe-481c-d9df-066d67e05356"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    "\n",
    "def remove_abbriviation(text):\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"'ll\", \" will\")  \n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    \n",
    "    text = text.replace(\"'ve\", \" have\")  \n",
    "    text = text.replace(\"have't\", \"have not\")\n",
    "    \n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    \n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"did't\", \"did not\")\n",
    "    text = text.replace(\"can't\", \"can not\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    return text\n",
    "\n",
    "def filtered(text):\n",
    "    # text = text.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
    "    # text = re.sub('\\s+',' ',text)       #repalces repeated whitespace characters with single space\n",
    "\n",
    "    # # text = re.sub('[/?@$->-_&]', '', text)\n",
    "    # # text = re.sub('[$->-_&]', '', text)\n",
    "    # text = remove_stopwords(text)\n",
    "    # text = stem_words(text)\n",
    "    \n",
    "    # REFERENCE: https://stackoverflow.com/questions/28840908/perfect-regex-for-extracting-url-with-re-findall\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \"\", text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    text = text.replace(\"@USER \", '')\n",
    "    text = text.replace(\"<URL>\", '')\n",
    "    text = text.lower()\n",
    "    text = remove_abbriviation(text)\n",
    "    text = re.sub(\"[,.\\\"\\'!@#$%^&*(){}+=-_?/;`~:<>\\\\\\[\\]]\", \"\", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def get_data():\n",
    "    train_text = []\n",
    "    train_labels = []\n",
    "    test_text = []\n",
    "    test_labels = []\n",
    "    with open('data/train.jsonl') as json_file: \n",
    "        for i in json_file:\n",
    "            data = json.loads(i)\n",
    "            if (data['label']) == 'SARCASM':\n",
    "              train_labels.append(1)\n",
    "            else:\n",
    "              train_labels.append(0)\n",
    "            train_text.append(filtered(data[\"response\"]))\n",
    "            # train_data.append({'label': data['label'], 'text':filtered(data[\"response\"])})\n",
    "    with open('data/test.jsonl') as json_file: \n",
    "        for i in json_file:\n",
    "            data = json.loads(i)\n",
    "            test_labels.append(int(data['id'].split(\"_\")[1]))\n",
    "            test_text.append(filtered(data[\"response\"]))\n",
    "            # test_data.append({'id': int(data['id'].split(\"_\")[1]), 'text':filtered(data[\"response\"])})\n",
    "    return train_text, train_labels, test_text, test_labels\n",
    "train_text, train_labels, test_text, test_labels = get_data()\n",
    "eval_text = train_text[:500] + train_text[-500:]\n",
    "eval_labels = train_labels[:500] + train_labels[-500:]\n",
    "train_text = train_text[500:-500]\n",
    "train_labels = train_labels[500:-500]\n",
    "\n",
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "_DZjYqHgtzTN",
    "outputId": "a00ada9b-505f-4c93-fe2a-3fce210e9e27"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUxJWfhbvFl1"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWTaIi6avIu_",
    "outputId": "3c1f2d2b-9607-4572-9d5a-dc355b2c041f"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer(\n",
    "    train_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "tokens_eval = tokenizer(\n",
    "    eval_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer(\n",
    "    test_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe3Ftid3wqlC",
    "outputId": "a26dcfde-f2a1-47fc-a433-9a89aa197d24"
   },
   "outputs": [],
   "source": [
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "print(train_seq.shape)\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels).to(device)\n",
    "print(train_y.shape)\n",
    "\n",
    "eval_seq = torch.tensor(tokens_eval['input_ids'])\n",
    "print(eval_seq.shape)\n",
    "eval_mask = torch.tensor(tokens_eval['attention_mask'])\n",
    "eval_y = torch.tensor(eval_labels).to(device)\n",
    "print(eval_y.shape)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "print(train_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "eval_data = TensorDataset(eval_seq, eval_mask, eval_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1-kYrHxwLzQ"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "      self.dropout1 = nn.Dropout(0.5)\n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,256)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(256,2)\n",
    "      # self.fc3 = nn.Linear(256,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      output= self.bert(sent_id, attention_mask=mask)\n",
    "      x = self.fc1(self.dropout1(output.last_hidden_state[:,0,:]))\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "      # x = self.dropout(self.relu(self.fc2(x)))\n",
    "      # x = self.dropout(self.relu(self.fc3(x)))\n",
    "      # x = self.dropout(self.relu(self.fc4(x)))\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoTxmAGuwOfU"
   },
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)\n",
    "criterion = nn.NLLLoss() \n",
    "# push the model to GPU\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1FbnA0NS4hU"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKRvdpGhwRdI"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  total_labels = []\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "    # outputs = model(sent_id, attention_mask=mask, labels=labels)\n",
    "    optimizer.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = criterion(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds.argmax(1))\n",
    "    total_labels.append(labels.cpu().numpy())\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds, np.concatenate(total_labels, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_eval(model):\n",
    "  \n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  total_labels = []\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(eval_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(eval_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "    \n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    total_preds.append(preds.argmax(1))\n",
    "    total_labels.append(labels.cpu().numpy())\n",
    "\n",
    "  \n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return total_preds, np.concatenate(total_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "PZeLMF7ywcvH",
    "outputId": "0ded17bd-385b-48da-b12a-b2c2aa3799c9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "epochs= 20\n",
    "best_valid_loss = float('inf')\n",
    "best_f1 = -10\n",
    "train_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss, total_preds, total_labels = train()\n",
    "    print(\"train_acc\", sum(total_preds == total_labels)/total_labels.shape[0])\n",
    "    preds, eval_labels = evaluate_eval(model)\n",
    "    curr_f1 = f1_score(eval_labels, preds)\n",
    "    print(\"eval_f1\", curr_f1)\n",
    "    print(\"eval_acc\", sum(eval_labels == preds)/preds.shape[0])\n",
    "    if curr_f1 > best_f1:\n",
    "      torch.save(model, \"/data/model12345.pt\")\n",
    "      best_f1 = curr_f1\n",
    "    # torch.save(model, \"/data/model123.pt\")\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate(model):\n",
    "  \n",
    "  model.eval()\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "\n",
    "  preds=preds.detach().argmax(1).cpu().numpy()\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29PCPd1cwgCP"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load(\"/data/model12345.pt\")\n",
    "# get predictions for test data\n",
    "preds, eval_labels = evaluate_eval(model)\n",
    "curr_f1 = f1_score(eval_labels, preds)\n",
    "print(\"eval_f1\", curr_f1)\n",
    "print(\"eval_acc\", sum(eval_labels == preds)/preds.shape[0])\n",
    "with torch.no_grad():\n",
    "  preds = evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoIhaHuT2DA1"
   },
   "outputs": [],
   "source": [
    "\n",
    "assert len(preds) == len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOXQ5bqb2Ke6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(\"answer.txt\", 'w+')\n",
    "for i in range(len(test_labels)):\n",
    "    if preds[i] == 0:\n",
    "        curr_pred = \"NOT_SARCASM\"\n",
    "    else:\n",
    "        curr_pred = \"SARCASM\"\n",
    "    f.write('twitter_{},{}\\n'.format(test_labels[i], curr_pred))\n",
    "    print('twitter_{},{}\\n'.format(test_labels[i], curr_pred))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7caYsHY2P2Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
